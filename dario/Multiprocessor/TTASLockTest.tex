\section{\textbf{TTASLockTest}}

\subsection{Particular Case (problem)}
Here we are trying to address the shortcomings of the \C{TASLock}
solution, which we know per the book causes a lot of bus traffic.

\subsection{Solution}
The solution is to loop on a locally cached variable, so that we do
not generate traffic on the bus. The code is simple enough and is
presented below:

\begin{lstlisting}[style=numbers]
public class TTASLock implements Lock {
  AtomicBoolean state = new AtomicBoolean(false);
  public void lock() {
    while (true) {
      while (state.get()) {};  // spin
      if (!state.getAndSet(true))
        return;
    }
  }
  public void unlock() {
    state.set(false);
  }
  ...
}
\end{lstlisting}
\hfill

Only the first time the \C{get} in the loop is called it generates bus
traffic, but after that and until the moment where the state is
changed by the owner of the lock, each thread uses its core cache. The
drawback of this solution though, is that traffic comes once the owner
releases the lock; in that moment the caches of all the cores waiting
are invalidated, generating bus traffic. Furthermore, once unleashed
all the threads/cores will call the synchronization primitive
\C{getAndSet} (which again will generate traffic on the bus). After
some fierce competition, once we have a winner, the system will reach
again an stable configuration (looser threads spinning on local
variables). 

\subsection{Experiment Description}
The test is exactly the same as that of \C{BackoffLockTest}: 8 threads
try to increment 1024 times a shared counter, and they do that
concurrently. At the end the counter shall have $8 * 1024$ as final
value. 

\subsection{Observations and Interpretations}

The test runs fine on 2 and 24 cores, below a sample successful
execution:

\begin{verbatim}
.parallel

Time: 0.024

OK (1 test)
\end{verbatim}
\hfill

We did not require to set to volatile any of the variables, as they were already
atomic (\C{AtomicBoolean} implies same guarantees than \C{volatile}, and even more).

